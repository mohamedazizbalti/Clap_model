{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":267422,"sourceType":"datasetVersion","datasetId":110374},{"sourceId":11499173,"sourceType":"datasetVersion","datasetId":7208876}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torchaudio\nimport torch\nfrom transformers import ClapModel, ClapProcessor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)\nprocessor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diagnosis_df = pd.read_csv('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv', header=None, names=['patient_id', 'diagnosis'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diagnosis_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_names = [\n    'Patient number',\n    'Age',\n    'Sex',\n    'Adult BMI (kg/m2)',\n    'Child Weight (kg)',\n    'Child Height (cm)'\n]\ndemo_path = \"/kaggle/input/respiratory-sound-database/demographic_info.txt\"\ndemographics_df = pd.read_csv(demo_path, sep='\\s+',header=None, names=column_names,)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"demographics_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_patient_description(row):\n    \"\"\"Generate a human-readable description of a patient with NaN handling\"\"\"\n    description = f\"Patient {row['Patient number']} is a \"\n    \n    # Age description with NaN check\n    if pd.isna(row['Age']):\n        description += \"patient with unknown age\"\n    else:\n        age = float(row['Age'])\n        if age < 1:\n            description += f\"{age*12:.1f}-month-old\"\n        elif age < 18:\n            description += f\"{int(age)}-year-old\"\n        else:\n            description += f\"{int(age)}-year-old adult\"\n    \n    # Sex description with NaN check\n    if pd.isna(row['Sex']):\n        description += \" of unspecified sex.\"\n    else:\n        description += f\" {row['Sex']}.\"\n    \n    # Adult BMI or child measurements with NaN checks\n    if pd.notna(row['Adult BMI (kg/m2)']):\n        description += f\" The adult has a BMI of {row['Adult BMI (kg/m2)']} kg/mÂ².\"\n    else:\n        child_info = []\n        if pd.notna(row['Child Weight (kg)']):\n            child_info.append(f\"weighs {row['Child Weight (kg)']} kg\")\n        if pd.notna(row['Child Height (cm)']):\n            child_info.append(f\"is {row['Child Height (cm)']} cm tall\")\n        \n        if child_info:\n            description += \" The child \" + \" and \".join(child_info) + \".\"\n    \n    return description\n\n# Load the data with proper NA handling\ncolumn_names = [\n    'Patient number',\n    'Age',\n    'Sex',\n    'Adult BMI (kg/m2)',\n    'Child Weight (kg)',\n    'Child Height (cm)'\n]\ndemo_path = \"/kaggle/input/respiratory-sound-database/demographic_info.txt\"\ndf = pd.read_csv(demo_path, sep='\\s+',header=None, names=column_names,)\n\n# Generate descriptions for all patients\ndf['Description'] = df.apply(generate_patient_description, axis=1)\n\n# Print all descriptions\nprint(\"Complete Patient Descriptions:\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Load Official Split\nwith open('/kaggle/input/others/official_split.txt') as f:\n    official_split = f.readlines()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_samples = [line.strip().split('\\t')[0] for line in official_split if \"train\" in line]\ntest_samples = [line.strip().split('\\t')[0] for line in official_split if \"test\" in line]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_samples)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Load Annotation Files\ndef load_annotation(annotation_file):\n    with open(annotation_file, 'r') as file:\n        lines = file.readlines()\n        cycle_data = []\n        for line in lines:\n            parts = line.split('\\t')  # Assuming tab-separated columns\n            cycle_data.append({\n                'start_time': float(parts[0]),\n                'end_time': float(parts[1]),\n                'crackles': int(parts[2]),\n                'wheezes': int(parts[3]),\n            })\n    return cycle_data\n\n# 4. Load Audio Files\ndef load_audio(audio_path):\n    waveform, sample_rate = torchaudio.load(audio_path)\n    return waveform, sample_rate\n\n# 5. Extract Relevant Audio Segments\ndef extract_breathing_segment(waveform, sample_rate, start_time, end_time):\n    start_sample = int(start_time * sample_rate)\n    end_sample = int(end_time * sample_rate)\n    return waveform[:, start_sample:end_sample]\n    \ndef get_label(crackles, wheezes):\n    if crackles == 1 and wheezes == 1:\n        return \"both\"\n    elif crackles == 1:\n        return \"crackles\"\n    elif wheezes == 1:\n        return \"wheezes\"\n    else:\n        return \"normal\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gather embeddings and labels\naudio_embeddings = []\ntext_embeddings = []\nlabels = []\ntarget_sample_rate = 48000\n\ndata_dir = '/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\naudio_embeddings = []\ntext_embeddings = []\nlabels = []\nfor file_id in tqdm(train_samples, desc=\"Processing AKGC417L samples\"):\n    if \"AKGC417L\" not in file_id:\n        continue  # Skip non-microphone recordings\n    audio_path = os.path.join(data_dir, file_id + '.wav')\n    annotation_path = os.path.join(data_dir, file_id + '.txt')\n    try:\n        description = (df[df[\"Patient number\"] == int(file_id[:3])][\"Description\"]).values[0]\n        text_inputs = processor(text=description, return_tensors=\"pt\", padding=True, truncation=True)\n        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n        with torch.no_grad():\n            text_embed = model.get_text_features(**text_inputs)\n    except Exception as e:\n        print(f\"[Text Error] {file_id}: {e}\")\n        continue\n\n    try:\n        waveform, sample_rate = load_audio(audio_path)\n        annotation = load_annotation(annotation_path)\n    except Exception as e:\n        print(f\"[Audio/Annotation Error] {file_id}: {e}\")\n        continue\n\n    for cycle in annotation:\n        try:\n            segment = extract_breathing_segment(waveform, sample_rate, cycle['start_time'], cycle['end_time'])\n            segment = segment.squeeze(0)\n\n            if sample_rate != target_sample_rate:\n                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n                segment = resampler(segment)\n\n            audio_inputs = processor(audios=segment, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n            audio_inputs = {k: v.to(device) for k, v in audio_inputs.items()}\n            with torch.no_grad():\n                audio_embed = model.get_audio_features(**audio_inputs)\n\n            if get_label(cycle['crackles'], cycle['wheezes']) not in [\"wheezes\",\"crackles\",\"normal\"]:\n                continue\n            else :\n                labels.append(get_label(cycle['crackles'], cycle['wheezes']))\n                audio_embeddings.append(audio_embed.squeeze(0))\n                text_embeddings.append(text_embed.squeeze(0))\n        except Exception as e:\n            print(f\"[Cycle Error] {file_id}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert lists to tensors\naudio_tensor = torch.stack(audio_embeddings)\ntext_tensor = torch.stack(text_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Combine embeddings and prepare data\nmultimodal_embeddings = torch.cat([audio_tensor, text_tensor], dim=1)\n\n# Ensure we're working with float32 tensors\nmultimodal_embeddings = multimodal_embeddings.float()\n\n# 2. Prepare labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(labels)  # Convert string labels to numerical\nnum_classes = len(label_encoder.classes_)\n\n# Convert to tensors - ensure proper types and devices\nX_tensor = multimodal_embeddings.cpu()  # Features on CPU\ny_tensor = torch.from_numpy(y_encoded).long()  # Labels as int64\n\n# 3. Create dataset and dataloaders\ndataset = TensorDataset(X_tensor, y_tensor)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False, pin_memory=True)\n\n# 4. Model definition\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64,num_classes)\n            \n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# Initialize model\ninput_dim = multimodal_embeddings.shape[1]\nmodel_classifier = MultimodalClassifier(input_dim, num_classes).to(device)\n\n# 5. Training setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model_classifier.parameters(), lr=1e-3, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Training loop with proper device management\nbest_val_acc = 0\nfor epoch in range(100):  # Or your desired number of epochs\n    model_classifier.train()\n    train_loss = 0\n    \n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model_classifier(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    scheduler.step()\n    \n    # Validation\n    model_classifier.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            outputs = model_classifier(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n    \n    val_acc = correct / total\n    print(f'Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, Val Acc: {val_acc:.4f}')\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model_classifier.state_dict(), 'best_multimodal_model.pth')\n\nprint(\"Training complete!\")\nprint(f\"Best validation accuracy: {best_val_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\n\ntest_audio_embeddings = []\ntest_text_embeddings = []\ntest_labels = []\ntarget_sample_rate = 48000\n\ndata_dir = '/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files'\n\nfor file_id in tqdm(test_samples, desc=\"Processing test samples\"):\n    audio_path = os.path.join(data_dir, file_id + '.wav')\n    annotation_path = os.path.join(data_dir, file_id + '.txt')\n\n    try:\n        description = (df[df[\"Patient number\"] == int(file_id[:3])][\"Description\"]).values[0]\n        text_inputs = processor(text=description, return_tensors=\"pt\", padding=True, truncation=True)\n        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n        with torch.no_grad():\n            text_embed = model.get_text_features(**text_inputs)\n    except Exception as e:\n        print(f\"[Text Error] {file_id}: {e}\")\n        continue\n\n    try:\n        waveform, sample_rate = load_audio(audio_path)\n        annotation = load_annotation(annotation_path)\n    except Exception as e:\n        print(f\"[Audio/Annotation Error] {file_id}: {e}\")\n        continue\n\n    for cycle in annotation:\n        try:\n            segment = extract_breathing_segment(waveform, sample_rate, cycle['start_time'], cycle['end_time'])\n            segment = segment.squeeze(0)\n\n            if sample_rate != target_sample_rate:\n                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n                segment = resampler(segment)\n\n            # Amplitude normalization\n            if segment.abs().max() > 0:\n                segment = segment / segment.abs().max()\n\n            audio_inputs = processor(audios=segment, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n            audio_inputs = {k: v.to(device) for k, v in audio_inputs.items()}\n            with torch.no_grad():\n                audio_embed = model.get_audio_features(**audio_inputs)\n\n            label = get_label(cycle['crackles'], cycle['wheezes'])\n            if label == \"both\":\n                continue\n\n            test_audio_embeddings.append(audio_embed.squeeze(0))\n            test_text_embeddings.append(text_embed.squeeze(0))\n            test_labels.append(label)\n\n        except Exception as e:\n            print(f\"[Cycle Error] {file_id}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TEST PROCESS (aligned with training)\n# 1. Combine test embeddings (same as training)\ntest_audio_tensor = torch.stack(test_audio_embeddings).float()  # Already on CPU\ntest_text_tensor = torch.stack(test_text_embeddings).float()    # Already on CPU\ntest_multimodal = torch.cat([test_audio_tensor, test_text_tensor], dim=1).float()  # [n_samples, 1024]\n\n# 2. Prepare test labels (using the SAME label encoder from training)\ntest_y_encoded = label_encoder.transform(test_labels)  # Don't use fit_transform() here!\ntest_y_tensor = torch.from_numpy(test_y_encoded).long()\n\n# 3. Create test dataset (on CPU)\ntest_dataset = TensorDataset(test_multimodal.cpu(), test_y_tensor.cpu())\n\n# 4. Create test loader (same params as validation loader)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,  # Never shuffle test data\n    pin_memory=True  # Only works with CPU tensors\n)\n\n# 5. Evaluation loop (same as validation)\nmodel_classifier.eval()\ntest_correct = 0\ntest_total = 0\nall_preds = []\nall_targets = []\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        # Move to device (this is where pin_memory helps)\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Forward pass\n        outputs = model_classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n        \n        # Update metrics\n        test_total += targets.size(0)\n        test_correct += (predicted == targets).sum().item()\n        \n        # Store for detailed metrics\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n# 6. Calculate metrics (same as training)\ntest_acc = test_correct / test_total\nprint(f'\\nTest Accuracy: {test_acc:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Detailed classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(\n    all_targets,\n    all_preds,\n    target_names=label_encoder.classes_\n))\n\n# Confusion matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(all_targets, all_preds)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_)\nplt.title('Test Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_multimodal.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multimodal_embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Example confusion matrix (replace with your actual matrix)\ncm = confusion_matrix(all_targets, all_preds)\nclasses = label_encoder.classes_\n\n# Initialize dictionaries to store metrics\nsensitivity = {}\nspecificity = {}\n\nfor i, class_name in enumerate(classes):\n    # True Positives (TP)\n    TP = cm[i, i]\n    \n    # False Negatives (FN)\n    FN = cm[i, :].sum() - TP\n    \n    # False Positives (FP)\n    FP = cm[:, i].sum() - TP\n    \n    # True Negatives (TN)\n    TN = cm.sum() - (TP + FP + FN)\n    \n    # Sensitivity (Recall)\n    sensitivity[class_name] = TP / (TP + FN) if (TP + FN) != 0 else 0\n    \n    # Specificity\n    specificity[class_name] = TN / (TN + FP) if (TN + FP) != 0 else 0\n\n# Print results\nprint(\"Sensitivity (Se) per class:\")\ne=0\nfor class_name, se in sensitivity.items():\n    e+=se\n    print(f\"{class_name}: {se:.3f}\")\nprint(f\"Avg: {e/4:.3f}\")\nprint(\"\\nSpecificity (Sp) per class:\")\np=0\nfor class_name, sp in specificity.items():\n    p+=sp\n    print(f\"{class_name}: {sp:.3f}\")\nprint(f\"Avg: {p/4:.3f}\")\nprint(f\"\\nScore: {(e+p)/8}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}